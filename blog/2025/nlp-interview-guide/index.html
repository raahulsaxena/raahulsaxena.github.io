<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> NLP Interview Guide - Key Concepts and Techniques | Rahul Saxena </title> <meta name="author" content="Rahul Saxena"> <meta name="description" content="A comprehensive guide to understanding core NLP concepts and techniques, including supervised learning, self-supervised learning, language models, and more."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://raahulsaxena.github.io/blog/2025/nlp-interview-guide/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Rahul</span> Saxena </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">NLP Interview Guide - Key Concepts and Techniques</h1> <p class="post-meta"> Created on March 14, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/tag/algorithms"> <i class="fa-solid fa-hashtag fa-sm"></i> Algorithms</a>   ·   <a href="/blog/category/natural-language-processing"> <i class="fa-solid fa-tag fa-sm"></i> Natural Language Processing</a>   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="supervised-learning">Supervised Learning</h3> <p>Given a collection of labeled examples (where each example is a text <code class="language-plaintext highlighter-rouge">X</code> paired with a label <code class="language-plaintext highlighter-rouge">Y</code>), learn a mapping from <code class="language-plaintext highlighter-rouge">X</code> to <code class="language-plaintext highlighter-rouge">Y</code>.</p> <h3 id="self-supervised-learning">Self-Supervised Learning</h3> <p>Given a collection of <em>just text</em>, without extra labels, <strong>create labels out of the text</strong> and use them for <em>pretraining</em> a model that has some general understanding of human language.</p> <ul> <li> <strong>Language Modeling</strong>: Given the beginning of a sentence or document, predict the next word.</li> <li> <strong>Masked Language Modeling</strong>: Given an entire document with some words or spans masked out, predict the missing words.</li> </ul> <h3 id="in-context-learning">In-context Learning</h3> <p>First pretrain a self-supervised model, and then prompt it in natural language to solve a particular task without any further understanding.</p> <p>Example: pretrain a LLM on billions of words, and then feed in *what is the sentiment of this sentence: <insert sentence="">*</insert></p> <ul> <li>Checkout <a href="http://api.together.xyz" rel="external nofollow noopener" target="_blank"><strong>api.together.xyz</strong></a> !</li> </ul> <h2 id="n-gram-language-models">N-gram Language Models</h2> <p>Let’s say we wanted to train a supervised model on sentiment analysis. In the past, we would have trained a supervised model on labeled examples (text/score pairs).</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nlp-guide/n-gram-1-480.webp 480w,/assets/img/nlp-guide/n-gram-1-800.webp 800w,/assets/img/nlp-guide/n-gram-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/nlp-guide/n-gram-1.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> N-gram Language Model </div> <p>Nowadays, we take advantage of <em>transfer learning</em>:</p> <div class="row mt-3 justify-content-center"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nlp-guide/n-gram-2-480.webp 480w,/assets/img/nlp-guide/n-gram-2-800.webp 800w,/assets/img/nlp-guide/n-gram-2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/nlp-guide/n-gram-2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Transfer Learning </div> <p>For n-gram models discussion, we focus on language modeling which form the core of <strong><em>Step 1: self-supervised pretraining.</em></strong></p> <h3 id="language-models-assign-a-probability-to-a-piece-of-text">Language Models Assign a Probability to a Piece of Text</h3> <ul> <li> <strong>Translation</strong>: Picking the most likely translation from a list of candidates. <ul> <li>P(i flew to the movies) «« P(i went to the movies)</li> </ul> </li> <li> <strong>Speech Recognition</strong>: <ul> <li>P(i saw a van) »» P(eyes awe of an)</li> </ul> </li> </ul> <p>Language models allow us to assign this probability to the words. Example: in search bars.</p> <h3 id="probabilistic-language-modeling">Probabilistic Language Modeling</h3> <ul> <li> <strong>Goal</strong>: Compute the probability of a sentence or sequence of words.</li> </ul> <h4 id="the-chain-rule">The Chain Rule</h4> <p><strong>Conditional Probability</strong>:</p> \[P(B | A) = \frac{P(A, B)}{P(A)}\] <p>Rewriting:</p> \[P(A, B) = P(A) P(B | A)\] <p>The chain rule in general:</p> \[P(x_1, x_2, x_3, ..., x_n) = P(x_1) P(x_2 | x_1) P(x_3 | x_1, x_2) ... P(x_n | x_1, ..., x_{n-1})\] <p>Chain Rule Applied to Compute Joint Probability of Words in a Sentence</p> \[P(w_1 w_2 \dots w_n) = \prod_{i} P(w_i | w_1 w_2 \dots w_{i-1})\] <p>Example Calculation:</p> <p>For the sentence <strong>“its water is so transparent”</strong>:</p> \[P(\text{"its water is so transparent"}) = P(\text{its}) \times P(\text{water} | \text{its}) \times P(\text{is} | \text{its water})\] \[\times P(\text{so} | \text{its water is}) \times P(\text{transparent} | \text{its water is so})\] <ul> <li> <strong>Unigram Model</strong>: Calculating the probability of “its” without any context. Frequency of <em>its</em> in the given corpus.</li> </ul> <p>Estimating these probabilities:</p> \[P(\text{the} | \text{its water is so transparent that}) = \frac{Count(\text{its water is so transparent that the})}{Count(\text{its water is so transparent that})}\] <ul> <li>This suffers from <strong>sparsity</strong> because the phrase “its water is so transparent that” might occur only a handful of times in the corpus.</li> <li>Also, we aren’t sharing information across semantically similar prefixes in this approach. All prefixes are treated independently, which is inefficient.</li> <li>Therefore, we use <strong>Markov Assumption</strong>: It will not consider words beyond a fixed window size (context length).</li> </ul> \[P(\text{the} | \text{its water is so transparent that}) \approx P(\text{the} | \text{transparent that})\] <ul> <li>Checkout <strong>infini-gram</strong> paper.</li> </ul> <h3 id="one-hot-vectors">One Hot Vectors</h3> <ul> <li>N-gram models rely on the “bag-of-words” assumption.</li> <li>Represent each word/n-gram as a vector of zeros with a single 1 identifying its index.</li> </ul> <div class="row mt-3 justify-content-center"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nlp-guide/one-hot-1-480.webp 480w,/assets/img/nlp-guide/one-hot-1-800.webp 800w,/assets/img/nlp-guide/one-hot-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/nlp-guide/one-hot-1.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> One Hot Vector </div> <ul> <li>All words are equally similar, even though “movies” and “film” are semantically similar. There is no information sharing between different words. All words are orthogonal.</li> <li>We ideally want a representation space in which words, phrases, sentences, etc. that are semantically similar have similar representations.</li> </ul> <h3 id="notion-of-perplexity">Notion of Perplexity</h3> <ul> <li> <strong>Lower perplexity = better model</strong>.</li> <li>Perplexity is the <em>exponentiated token-level negative log-likelihood</em>.</li> <li>Given a prefix, how many next words are reasonable predictions given that prefix.</li> </ul> \[\text{Perplexity}(\mathcal{W}) = e^{ - \frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_1, w_2, ..., w_{i-1}) }\] <table> <thead> <tr> <th>N-gram Order</th> <th>Unigram</th> <th>Bigram</th> <th>Trigram</th> </tr> </thead> <tbody> <tr> <td>Perplexity</td> <td>962</td> <td>170</td> <td>109</td> </tr> </tbody> </table> <h2 id="neural-language-models">Neural Language Models</h2> <div class="row mt-3 justify-content-center"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nlp-guide/nlm-1-480.webp 480w,/assets/img/nlp-guide/nlm-1-800.webp 800w,/assets/img/nlp-guide/nlm-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/nlp-guide/nlm-1.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Neural Language Model </div> <h3 id="words-as-basic-building-blocks">Words as Basic Building Blocks</h3> <ul> <li>Represent words with low-dimensional vectors called embeddings.</li> </ul> <div class="row mt-3 justify-content-center"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nlp-guide/nlm-2-480.webp 480w,/assets/img/nlp-guide/nlm-2-800.webp 800w,/assets/img/nlp-guide/nlm-2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/nlp-guide/nlm-2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Words as Basic Building Blocks </div> <ul> <li>There is a loss of interpretation with these numbers. [Research field of explainability]</li> <li>The word embeddings start completely randomly.</li> <li>Neural networks compose word embeddings into vectors for phrases, sentences, and documents. It uses word embeddings to find representations for phrases and sentences.</li> </ul> <div class="row mt-3 justify-content-center"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nlp-guide/nlm-3-480.webp 480w,/assets/img/nlp-guide/nlm-3-800.webp 800w,/assets/img/nlp-guide/nlm-3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/nlp-guide/nlm-3.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> </div> <ul> <li>Predict the next word from composed prefix representation.</li> </ul> <div class="row mt-3 justify-content-center"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nlp-guide/nlm-4-480.webp 480w,/assets/img/nlp-guide/nlm-4-800.webp 800w,/assets/img/nlp-guide/nlm-4-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/nlp-guide/nlm-4.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> </div> <h3 id="linear-layer-feedforward-layer-on-the-prefix-vector-representation">Linear Layer (Feedforward Layer) on the Prefix Vector Representation</h3> <div class="row mt-3 justify-content-center"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nlp-guide/ll-1-480.webp 480w,/assets/img/nlp-guide/ll-1-800.webp 800w,/assets/img/nlp-guide/ll-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/nlp-guide/ll-1.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Feed-forward Layer </div> <p>How to go from vector representation (Wx) to a probability distribution of the next word?</p> <ul> <li> <strong>Softmax Layer</strong>: Converts a vector representation into a probability distribution over the entire vocabulary.</li> </ul> <div class="row mt-3 justify-content-center"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nlp-guide/softmax-2-480.webp 480w,/assets/img/nlp-guide/softmax-2-800.webp 800w,/assets/img/nlp-guide/softmax-2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/nlp-guide/softmax-2.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Softmax </div> <h3 id="different-types-of-composition-functions">Different Types of Composition Functions</h3> <p><em>Input</em>: sequence of word embeddings corresponding to the tokens of a given prefix.</p> <p><em>Output</em>: single vector.</p> <ul> <li>Element-wise functions (e.g., just sum up all the embeddings)</li> <li>Concatenation</li> <li>Feed-forward neural networks</li> <li>RNNs</li> <li>Transformers</li> </ul> <h3 id="a-fixed-window-neural-language-model">A Fixed Window Neural Language Model</h3> <div class="row mt-3 justify-content-center"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nlp-guide/fixed-window-1-480.webp 480w,/assets/img/nlp-guide/fixed-window-1-800.webp 800w,/assets/img/nlp-guide/fixed-window-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/nlp-guide/fixed-window-1.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fixed Window Neural Language Model </div> <ul> <li> <em>ReLU</em> non-linearity = max(0, x). Others include <em>tanh</em> and <em>sigmoid</em>.</li> <li> <em>f</em> in the above figure is a non-linear function. We want to model non-linear relationships, and therefore there is a need for non-linear functions.</li> </ul> <h3 id="how-does-this-compare-to-normal-n-gram-model">How Does This Compare to Normal N-gram Model?</h3> <p><strong>Improvements</strong>:</p> <ul> <li>No sparsity problem.</li> <li>Model size is O(n), not O(exp(n)).</li> </ul> <p><strong>Remaining Problems</strong>:</p> <ul> <li>Fixed window size is very small.</li> <li>Enlarging window size enlarges <strong><em>W</em></strong>.</li> <li>Window can never be large enough!</li> <li>Each Ci uses different rows of <strong><em>W</em></strong>. We don’t share weights across the window.</li> </ul> <p>These remaining problems are addressed by <strong>Recurrent Neural Networks</strong>.</p> <h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2> <p>It is <strong>sequential</strong> and does the calculation from left to right [words of prefix].</p> <ul> <li>It addresses the problem of not sharing the weights across the window.</li> <li>It also takes care of the Markov assumption. Theoretically, there is no limit to <code class="language-plaintext highlighter-rouge">t</code> in the RNN equation. We can keep on calculating <code class="language-plaintext highlighter-rouge">h(t)</code>.</li> </ul> <div class="row mt-3 justify-content-center"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nlp-guide/rnn-1-480.webp 480w,/assets/img/nlp-guide/rnn-1-800.webp 800w,/assets/img/nlp-guide/rnn-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/nlp-guide/rnn-1.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Recurrent Neural Network </div> <h3 id="rnn-advantages">RNN Advantages:</h3> <ul> <li>Can process any length input.</li> <li>Model size doesn’t increase for longer input.</li> <li>No limit on the size of the window (theoretically). Can use information from many steps back.</li> <li>Weights are shared across timestamps → representations are shared.</li> </ul> <h3 id="rnn-disadvantages">RNN Disadvantages:</h3> <ul> <li>Recurrent computation is slow.</li> <li>In practice, difficult to access information from many steps back.</li> </ul> <h2 id="neural-language-models-backpropagation">Neural Language Models [Backpropagation]</h2> <h3 id="training-neural-language-models">Training Neural Language Models</h3> <ul> <li>Last time we talked about forward propagation.</li> <li>How can we make NLMs make good predictions?</li> <li>NLMs contain parameters (e.g., <code class="language-plaintext highlighter-rouge">W1</code>, <code class="language-plaintext highlighter-rouge">W2</code>, <code class="language-plaintext highlighter-rouge">c1</code>, <code class="language-plaintext highlighter-rouge">c2</code>…) <ul> <li>These parameters are randomly initialized.</li> <li> <table> <tbody> <tr> <td>Thus, $P(w_1 w_2 \dots w_n) = \prod_{i} P(w_i</td> <td>w_1 w_2 \dots w_{i-1})$ is also randomized initially.</td> </tr> </tbody> </table> </li> </ul> </li> <li>By training the NLM, we adjust its parameters to maximize the likelihood of the training data.</li> </ul> <p><strong>Recap of Forward Propagation</strong></p> <div class="row mt-3 justify-content-center"> <div class="col-sm-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/nlp-guide/forward-1-480.webp 480w,/assets/img/nlp-guide/forward-1-800.webp 800w,/assets/img/nlp-guide/forward-1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/nlp-guide/forward-1.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Forward Propagation </div> <h3 id="steps-to-train-a-neural-language-model">Steps to Train a Neural Language Model</h3> <ol> <li>Define a loss function <strong>L(ϴ)</strong>, where ϴ refers to all the parameters of the model. <ul> <li>This tells us how bad the model is at predicting the next word.</li> <li>Loss function is required to be smooth and differentiable.</li> </ul> </li> <li>Given the loss function <strong>L(ϴ)</strong>, we compute the gradient of L with respect to ϴ. <ul> <li>The gradient provides us the direction of steepest ascent of the loss function.</li> <li>Step in negative gradient will minimize the loss function. <strong>That’s the goal</strong>!</li> <li>Gradient is same dimensionality as ϴ.</li> </ul> </li> <li>Given the gradient <code class="language-plaintext highlighter-rouge">dL/dϴ</code>, we take a step in the direction of the negative gradient.</li> </ol> \[\theta_{\text{new}} = \theta_{\text{old}} - \eta \cdot \nabla_{\theta} L\] <p>Where:</p> <ul> <li>$\theta$ represents the model parameters</li> <li>$\eta$ is the learning rate</li> <li>$\nabla_{\theta} L$ is the gradient of the loss function with respect to $\theta$</li> </ul> <p><strong>Optimizers</strong>:</p> <ul> <li>Gradient Descent parameter: The one shown above is SGD (stochastic gradient descent).</li> <li>Other optimizers include Adam, Sophia (LLMs).</li> </ul> <p><strong>Hyperparameters of Gradient Descent</strong>:</p> <ul> <li>Learning rate</li> <li>Batch size</li> </ul> <p><strong>Loss Function</strong>:</p> <ul> <li>Loss function: Cross-entropy loss, negative log-likelihood of the next word from the training data given the prefix.</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/knapsack-like/">Mastering Knapsack The Dynamic Programming Patterns You Need to Know</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/binary-tree/">Branching Out A Deep Dive into Binary Trees</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/graphs/">Graph Algorithms Deep Dive Key Concepts and Techniques</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Rahul Saxena. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>