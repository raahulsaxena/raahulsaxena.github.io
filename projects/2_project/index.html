<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Prompt Score | Rahul Saxena </title> <meta name="author" content="Rahul Saxena"> <meta name="description" content="CS 685 course project"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://raahulsaxena.github.io/projects/2_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Rahul</span> Saxena </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Prompt Score</h1> <p class="post-description">CS 685 course project</p> </header> <article> <p><strong>Authors:</strong> Rahul Saxena, Amritansh Mishra, Manish Ranjan Karna, Manas Wadhwa <strong>Institution:</strong> UMass Amherst <a href="https://github.com/raahulsaxena/prompt_score" rel="external nofollow noopener" target="_blank">GitHub Link</a></p> <h2 id="abstract">Abstract</h2> <p>In this project, we introduce <strong>PromptScore</strong>, a novel mechanism designed to evaluate the specificity of prompts used in interacting with Large Language Models (LLMs) like GPT-4. Our goal is to create a standardized scoring system that assesses the effectiveness of prompts based on metrics such as coherence, clarity, and ambiguity. This scoring system, validated by human annotators, aims to provide a deeper understanding of LLM capabilities and performance, particularly in handling specific and complex prompts.</p> <h2 id="problem-statement">Problem Statement</h2> <p>The effectiveness of prompts significantly impacts the performance of LLMs. However, the lack of a standardized prompt evaluation metric poses challenges in optimizing prompt design for specific use cases. Our project addresses this gap by developing a prompt scoring system that evaluates prompt specificity and its impact on LLM performance. We explored two approaches: fine-tuning the LLaMA model using the QLoRA technique and leveraging GPT-4’s few-shot prompting strategy to generate prompt scores.</p> <h2 id="methodology">Methodology</h2> <h3 id="dataset-creation">Dataset Creation</h3> <p>We curated a dataset comprising 800 prompts, each annotated with scores based on four criteria:</p> <ul> <li> <strong>Number of Constraints:</strong> The total number of constraints in the prompt.</li> <li> <strong>Constraint Complexity:</strong> The complexity of each individual constraint.</li> <li> <strong>Clarity:</strong> The clarity and understandability of the prompt.</li> <li> <strong>Prompt Complexity:</strong> The overall complexity of the prompt’s storyline or structure.</li> </ul> <h3 id="model-fine-tuning">Model Fine-Tuning</h3> <ol> <li> <p><strong>LLaMA Fine-Tuning:</strong><br> We fine-tuned the LLaMA model using the QLoRA approach on our custom dataset. Despite optimizing hyperparameters such as learning rate and batch size, LLaMA’s performance on prompt evaluation remained suboptimal compared to GPT-4.</p> </li> <li> <p><strong>GPT-4 Few-Shot Learning:</strong><br> We used GPT-4 in a few-shot learning configuration to evaluate prompts. GPT-4 outperformed LLaMA, demonstrating a better understanding of the evaluation criteria and generating more coherent and high-quality prompt evaluations.</p> </li> </ol> <h3 id="evaluation-metrics">Evaluation Metrics</h3> <p>To evaluate the generated stories, we instructed GPT-4 to assess each story based on the following criteria:</p> <ul> <li> <strong>Coherence:</strong> Logical consistency and narrative flow.</li> <li> <strong>Constraints Satisfied:</strong> Adherence to the specific constraints outlined in the prompt.</li> <li> <strong>Fluency:</strong> Readability and linguistic quality.</li> </ul> <h2 id="results">Results</h2> <p>Our experiments revealed that GPT-4’s few-shot learning approach significantly outperformed the fine-tuned LLaMA model in generating accurate prompt scores. The results indicated that as prompt specificity increases, the performance of LLMs deteriorates, particularly in maintaining coherence and satisfying constraints.</p> <h3 id="key-findings">Key Findings</h3> <ul> <li> <strong>LLaMA Model:</strong> Struggled with nuanced prompt evaluation, particularly in coherence and constraints.</li> <li> <strong>GPT-4:</strong> Demonstrated superior performance in evaluating complex and specific prompts, validating the effectiveness of the PromptScore system.</li> </ul> <h2 id="error-analysis">Error Analysis</h2> <p>We identified a limitation in the model’s ability to score highly specific prompts accurately. For instance, prompts with intricate constraints tended to receive lower scores than expected, suggesting that our model may require further fine-tuning or a larger, more diverse dataset to improve accuracy.</p> <h2 id="conclusion">Conclusion</h2> <p>Our PromptScore system offers a standardized approach to evaluating prompt specificity and its impact on LLM performance. The findings suggest that GPT-4 is better equipped to handle complex prompts, making it a more reliable model for generating and evaluating specific language tasks. Future work will focus on expanding the dataset, refining the scoring system, and exploring alternative models to enhance prompt evaluation accuracy.</p> <h2 id="future-work">Future Work</h2> <ul> <li> <strong>Expand the Evaluation Dataset:</strong> Increase the number of prompts with varying specificity to calculate more robust performance metrics.</li> <li> <strong>Explore Alternative Scoring Methods:</strong> Investigate other LLMs or continuous scoring systems for more accurate and nuanced evaluations.</li> <li> <strong>Apply PromptScore to Other Domains:</strong> Generalize the scoring system to assess prompts across different domains and tasks.</li> </ul> </article> <h2>References</h2> <div class="publications"> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Rahul Saxena. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>